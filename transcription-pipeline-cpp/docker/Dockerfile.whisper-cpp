# Clean whisper.cpp setup with CUDA support
FROM nvidia/cuda:12.2.0-devel-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV CUDA_DOCKER_ARCH=all
ENV WHISPER_CUDA=1

# Use Australian mirror (since you're in Melbourne)
RUN sed -i 's|http://archive.ubuntu.com/ubuntu|http://au.archive.ubuntu.com/ubuntu|g' /etc/apt/sources.list && \
    sed -i 's|http://security.ubuntu.com/ubuntu|http://au.archive.ubuntu.com/ubuntu|g' /etc/apt/sources.list

# Install dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    git \
    cmake \
    libopenblas-dev \
    pkg-config \
    wget \
    ffmpeg \
    python3 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Set CUDA library paths for building (including stubs)
ENV LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:$LIBRARY_PATH

# Create symbolic link for libcuda stub (needed for building)
RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1

# Clone and build whisper.cpp with CUDA support
RUN git clone https://github.com/ggerganov/whisper.cpp.git && \
    cd whisper.cpp && \
    CUDACXX=/usr/local/cuda/bin/nvcc \
    cmake -B build \
        -DGGML_CUDA=ON \
        -DCMAKE_CUDA_ARCHITECTURES="60;70;75;80;86" \
        -DCMAKE_BUILD_TYPE=Release && \
    cmake --build build --config Release -j $(nproc) && \
    echo "Checking for CUDA support in binary:" && \
    ldd build/bin/whisper-cli | grep -i cuda || echo "Warning: No CUDA libraries linked"

# Remove stub libraries after building
RUN rm -f /usr/local/cuda/lib64/stubs/libcuda.so.1

# Download models
WORKDIR /app/whisper.cpp
RUN bash ./models/download-ggml-model.sh tiny.en && \
    bash ./models/download-ggml-model.sh base.en && \
    bash ./models/download-ggml-model.sh small.en && \
    bash ./models/download-ggml-model.sh medium.en

# Install Python dependencies
RUN pip3 install pyyaml

# Copy the transcription wrapper script
WORKDIR /app
COPY transcribe.py /app/transcribe.py
RUN chmod +x /app/transcribe.py

# Test nvidia-smi is available
RUN which nvidia-smi || echo "nvidia-smi not found in PATH"

# Set runtime LD_LIBRARY_PATH (exclude stubs directory)
ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

# Set entrypoint
ENTRYPOINT ["python3", "/app/transcribe.py"]
CMD ["--help"]