# Clean whisper.cpp setup with CUDA support
FROM nvidia/cuda:12.2.0-devel-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV CUDA_DOCKER_ARCH=all
ENV WHISPER_CUDA=1

# Install dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    git \
    cmake \
    libopenblas-dev \
    pkg-config \
    wget \
    ffmpeg \
    python3 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /app

# Clone and build whisper.cpp with CUDA support
RUN git clone https://github.com/ggerganov/whisper.cpp.git && \
    cd whisper.cpp && \
    make clean && \
    WHISPER_CUDA=1 make -j$(nproc)

# Download models
WORKDIR /app/whisper.cpp
RUN bash ./models/download-ggml-model.sh tiny.en && \
    bash ./models/download-ggml-model.sh base.en && \
    bash ./models/download-ggml-model.sh small.en && \
    bash ./models/download-ggml-model.sh medium.en && \
    bash ./models/download-ggml-model.sh large-v3

# Convert large-v3 to quantized format for 6GB GPUs
RUN ./quantize models/ggml-large-v3.bin models/ggml-large-v3-q5_0.bin q5_0

# Install Python dependencies
RUN pip3 install pyyaml

# Copy the transcription wrapper script
WORKDIR /app
COPY transcribe.py /app/transcribe.py
RUN chmod +x /app/transcribe.py

# Test nvidia-smi is available
RUN which nvidia-smi || echo "nvidia-smi not found in PATH"

# Set entrypoint
ENTRYPOINT ["python3", "/app/transcribe.py"]
CMD ["--help"]